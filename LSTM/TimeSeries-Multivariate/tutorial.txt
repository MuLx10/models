/*!
@file tutorial.txt
@author Mehul Kumar Nirala
@brief Tutorial on Multivariate Time Series using RNN.

@page rnntutorial LSTM Multivariate Time Series

@section intro_lstmtut Introduction

We want to use the power of the LSTM in Google stock prediction using time series. We will use mlpack and Recurrent Neural Network(RNN).
I have downloaded the Google stock prices for past 3 years from https://www.nasdaq.com/symbol/goog/historical to a csv file Google2016-2019.csv

@section toc_lstmtut Table of Contents

This tutorial is split into the following sections:

 - \ref intro_lstmtut
 - \ref toc_lstmtut
 - \ref data_lstmtut
 - \ref model_lstmtut
 - \ref training_lstmtut
 - \ref results_lstmtut

@section data_lstmtut Time Series data
 Initially we normalize the input data using MinMaxScaler so that all the input features are on the scale from 0 to 1.

@code
  template<typename DataType = arma::mat>
  DataType MinMaxScaler(DataType& dataset)
  {
    arma::vec maxValues = arma::max(dataset, 1 /* for each dimension */);
    arma::vec minValues = arma::min(dataset, 1 /* for each dimension */);

    arma::vec rangeValues = maxValues - minValues;

    // Add a very small value if there are any zeros.
    rangeValues += 1e-25;

    dataset -= arma::repmat(minValues , 1, dataset.n_cols);
    dataset /= arma::repmat(rangeValues , 1, dataset.n_cols);
    return dataset;
  }
  ...
  // Scale data for increased numerical stability.
  dataset = MinMaxScaler(dataset);
@endcode

 * This is a time series problem. If we need to predict the Google stock prices correctly then we need to consider the volume of the stocks traded from the previous days as well as the average stock prices from previous days.
 * We will be creating the data that will go back to 25 business days in past for the prediction.
 * Also, we will take 30 % of the latest data as our test dataset.
 * For RNN LSTM to predict the data we need to convert the input data.
 * Input data is in the form: [Volume of stocks traded, Opening stock price, Closing stock price, Min stock price, Max stock price] and we need to create a time series data.

The time series data for today should contain the [Volume of stocks traded, Opening stock price, Closing stock price, Min stock price, Max stock price] for past 25 days and the target variable will be Google’s stock price today and so on.
As the stock price prediction is based on multiple input features, it is a multivariate regression problem.
We loop through all the samples and for each day we go back 25 business days in the past and add the volume of the stocks traded an average stock price.

@code
  /*
   * The time series data for today should contain the [Volume of stocks traded,
   * Opening stock price, Closing stock price, Min stock price, Max stock price]
   * for past 'rho' days and the target variable will be Google’s
   * stock price today (high, low) and so on.
   */
  template<typename InputDataType = arma::mat,
           typename DataType = arma::cube,
           typename LabelType = arma::cube>
  void CreateTimeSeriesData(InputDataType dataset, DataType& X, LabelType& y, size_t rho)
  {
    for(size_t i = 0;i < dataset.n_cols - rho - 1 ;i++)
    {
      X.subcube(span(), span(i), span()) = dataset.submat(span(), span(i, i+rho-1));
      y.subcube(span(), span(i), span()) = dataset.submat(span(3,4), span(i, i+rho-1));
    }
  }
@endcode

@section model_lstmtut LSTM Model

We add 4 LSTM cells that will be stacked one after the other in the RNN, implementing an efficient stacked RNN. Finally, the output will have only one unit as this is a regression problem.

@code
  // No of timesteps to look in RNN.
  const int rho = 25;
  size_t inputSize = 5, outputSize = 2;

  // RNN model.
  RNN<MeanSquaredError<>,HeInitialization> model(rho);
  model.Add<IdentityLayer<> >();
  model.Add<LSTM<> > (inputSize, 10, maxRho);
  model.Add<Dropout<> >(0.5);
  model.Add<LeakyReLU<> >();
  model.Add<LSTM<> > (10, 10, maxRho);
  model.Add<Dropout<> >(0.5);
  model.Add<LeakyReLU<> >();
  model.Add<LSTM<> > (10, 10, maxRho);
  model.Add<LeakyReLU<> >();
  model.Add<Linear<> >(10, outputSize);

@endcode

Setting parameters Stochastic Gradient Descent (SGD) optimizer.
@code

  // Setting parameters Stochastic Gradient Descent (SGD) optimizer.
  SGD<AdamUpdate> optimizer(
    STEP_SIZE, // Step size of the optimizer.
    BATCH_SIZE, // Batch size. Number of data points that are used in each iteration.
    ITERATIONS_PER_EPOCH, // Max number of iterations.
    1e-8,// Tolerance.
    true,// Shuffle.
    AdamUpdate(1e-8, 0.9, 0.999)// Adam update policy.
  );

@endcode

@section training_lstmtut Training the model

@code
  cout << "Training ..." << endl;
  // Cycles for monitoring the process of a solution.
  for (int i = 0; i < EPOCH; i++)
  {
    // Train neural network. If this is the first iteration, weights are
    // random, using current values as starting point otherwise.
    model.Train(trainX, trainY, optimizer);

    // Don't reset optimizer's parameters between cycles.
    optimizer.ResetPolicy() = false;

    cube predOut;
    // Getting predictions on test data points.
    model.Predict(testX, predOut);

    // Calculating mse on test data points.
    double testMSE = MSE(predOut,testY);
    cout << i+1 << " - Mean Squared Error := "<< testMSE <<   endl;
  }
@endcode

@section results_lstmtut Results

Mean Squared error upto 100 iterations.
 1 - Mean Squared Error := 0.236375
 2 - Mean Squared Error := 0.233769
 3 - Mean Squared Error := 0.2336
 4 - Mean Squared Error := 0.12196
 5 - Mean Squared Error := 0.0666709
 6 - Mean Squared Error := 0.143383
 7 - Mean Squared Error := 0.14387
 8 - Mean Squared Error := 0.143908
 9 - Mean Squared Error := 0.143437
10 - Mean Squared Error := 0.17364
11 - Mean Squared Error := 0.173863
12 - Mean Squared Error := 0.173103
13 - Mean Squared Error := 0.172775
14 - Mean Squared Error := 0.173811
15 - Mean Squared Error := 0.173757
16 - Mean Squared Error := 0.173243
17 - Mean Squared Error := 0.172399
18 - Mean Squared Error := 0.172358
19 - Mean Squared Error := 0.172354
20 - Mean Squared Error := 0.172286
21 - Mean Squared Error := 0.164439
22 - Mean Squared Error := 0.164439
23 - Mean Squared Error := 0.15857
24 - Mean Squared Error := 0.115357
25 - Mean Squared Error := 0.126169
26 - Mean Squared Error := 0.170857
27 - Mean Squared Error := 0.102091
28 - Mean Squared Error := 0.102131
29 - Mean Squared Error := 0.101352
30 - Mean Squared Error := 0.0871095
31 - Mean Squared Error := 0.0915397
32 - Mean Squared Error := 0.0608284
33 - Mean Squared Error := 0.101486
34 - Mean Squared Error := 0.118897
35 - Mean Squared Error := 0.157499
36 - Mean Squared Error := 0.123698
37 - Mean Squared Error := 0.120925
38 - Mean Squared Error := 0.120485
39 - Mean Squared Error := 0.122089
40 - Mean Squared Error := 0.123703
41 - Mean Squared Error := 0.121248
42 - Mean Squared Error := 0.115864
43 - Mean Squared Error := 0.111401
44 - Mean Squared Error := 0.199107
45 - Mean Squared Error := 0.198944
46 - Mean Squared Error := 0.198938
47 - Mean Squared Error := 0.198931
48 - Mean Squared Error := 0.19892
49 - Mean Squared Error := 0.198593
50 - Mean Squared Error := 0.152314
51 - Mean Squared Error := 0.198554
52 - Mean Squared Error := 0.170493
53 - Mean Squared Error := 0.190916
54 - Mean Squared Error := 0.190563
55 - Mean Squared Error := 0.188142
56 - Mean Squared Error := 0.188204
57 - Mean Squared Error := 0.1849
58 - Mean Squared Error := 0.181032
59 - Mean Squared Error := 0.164479
60 - Mean Squared Error := 0.151979
61 - Mean Squared Error := 0.152478
62 - Mean Squared Error := 0.18466
63 - Mean Squared Error := 0.180347
64 - Mean Squared Error := 0.181344
65 - Mean Squared Error := 0.161524
66 - Mean Squared Error := 0.141533
67 - Mean Squared Error := 0.144807
68 - Mean Squared Error := 0.166642
69 - Mean Squared Error := 0.148973
70 - Mean Squared Error := 0.203383
71 - Mean Squared Error := 0.175114
72 - Mean Squared Error := 0.153967
73 - Mean Squared Error := 0.18701
74 - Mean Squared Error := 0.200092
75 - Mean Squared Error := 0.14802
76 - Mean Squared Error := 0.143452
77 - Mean Squared Error := 0.143236
78 - Mean Squared Error := 0.143233
79 - Mean Squared Error := 0.1122285
80 - Mean Squared Error := 0.1106924
81 - Mean Squared Error := 0.1116105
82 - Mean Squared Error := 0.1121355
83 - Mean Squared Error := 0.1118596
84 - Mean Squared Error := 0.1196523
85 - Mean Squared Error := 0.1100242
86 - Mean Squared Error := 0.1192389
87 - Mean Squared Error := 0.1185851
88 - Mean Squared Error := 0.1174079
89 - Mean Squared Error := 0.1161247
90 - Mean Squared Error := 0.1171614
91 - Mean Squared Error := 0.1139373
92 - Mean Squared Error := 0.137471
93 - Mean Squared Error := 0.146974
94 - Mean Squared Error := 0.145858
95 - Mean Squared Error := 0.143716
96 - Mean Squared Error := 0.144713
97 - Mean Squared Error := 0.143604
98 - Mean Squared Error := 0.150909
99 - Mean Squared Error := 0.147452

MSE Plot
![Figure_1-1](https://user-images.githubusercontent.com/23444642/61946563-92069f80-afc0-11e9-9788-c396376a6aa8.png)

*/
